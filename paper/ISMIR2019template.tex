% -----------------------------------------------
% Template for ISMIR Papers
% 2019 version, based on previous ISMIR templates

% Requirements :
% * 6+n page length maximum
% * 4MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage[T1]{fontenc} % add special characters (e.g., umlaute)
\usepackage[utf8]{inputenc} % set utf-8 as default input encoding
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}

% Optional: To use hyperref, uncomment the following.
% \usepackage[bookmarks=false,hidelinks]{hyperref}
% Mind the bookmarks=false option; bookmarks are incompatible with ismir.sty.

\newcommand{\setNameUpper}{FOURIER}
\newcommand{\setName}{Fourier}

% Title.
% ------
\title{The \setNameUpper~Set: Beats, Downbeats, and Functional Segment Annotations of Western Popular Music}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author}


% Three addresses
% --------------
\threeauthors
  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author, Third Author}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}
%\def\authorname{First author, Second author, Third author, Fourth author, Fifth author, Sixth author}


\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle
%
\begin{abstract}
    We introduce the \setName\footnote{Temporary name for the blind review process.}~set: a collection of annotations of beats, downbeats, and functional segmentation for over 700 tracks that covers a wide range of western popular music.
    Given the variety of annotated music information types in this set, and how strongly these three types of data are typically intertwined, we believe it could potentially foster research that focuses on multiple retrieval tasks at once.
    The dataset includes additional metadata such as MusicBrainz identifiers to support the linking of the dataset to third-party information or audio data when available.
    We describe the methodology employed in acquiring this set, including the annotation process and song selection. 
    In addition an initial data exploration of the annotations and actual dataset content is conducted. 
    Finally, we provide a series of baselines of the \setName~set with standard beat-trackers, downbeat predictors, and structural segmentation algorithms in the literature.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

The tasks of beat detection, downbeat prediction, and structural segmentation~\cite{Paulus2010} constitute a fundamental part of the field of MIR.
These three musical characteristics are often related: downbeats define the first beat of a given music measure, and long structural music segments tend to begin and end on specific beat locations --frequently on downbeats.
The automatic estimation of such information could result in better musical systems such as more accurate automatic DJ-ing, better intra- and inter-song navigation, further musicological insights of large collections, \emph{etc}.
While several approaches exploiting more than one of these musical traits have been proposed~\cite{Bock2016, Mccallum2019, Fuentes2019}, the lack of human annotated data containing the three of them for a single collection are scarce.
This limits the amount of potential of certain methods, especially those that require large amounts of information (e.g., deep learning~\cite{Humphrey2012}).

In this paper we present the \setName~set: human annotations of beats, downbeats, and functional segmentation for over 700 tracks of western popular music.
These annotations were gathered with the aim of having a significant amount of data to train models to improve the prediction of such musical attributes, which would later be applied to various products offered by the company \setName.
By releasing this set to the public, it is our aim to let the research community explore and exploit these annotations to advance the tasks of beat tracking, downbeat prediction, and automatic functional structural segmentation.
We discuss the methodology to acquire these data, including the song selection process, and the inclusion of standard identifiers (Acoustid and MusicBrainz) and onsets for the first 30 seconds of the tracks to allow for other researchers to more easily access and align, when needed, the actual audio content.
Furthermore, we present a series of results with standard algorithmic approaches in the literature with the goal of having an initial public benchmark of this set.

The rest of this work is organized as follows: Section~\ref{sec:background} contains a review of the most relevant public datasets of the tasks at hand; Section~\ref{sec:dataset} discusses the actual \setName~set, including the data gathering, their formatting, and various statistics; Section~\ref{sec:results} presents various benchmarks in the set; and Section~\ref{sec:conclusions} draws some final conclusions and discloses future remarks.
%
\section{Background}\label{sec:background}

In this section we discuss previously published datasets and the tasks they aim to solve.

\input{beat_downbeat.tex}

\subsection{Structural Segmentation Sets}

The task of structural segmentation has been particularly active in the MIR community since the late 2000s.
Similarly to the beat tracking task, several datasets have been published, and some of them have evolved over time.
This task is often divided into two subtasks: segment boundary retrieval and segment labeling.
All well-known published datasets contain both boundary and label information.
One of the major challenges with structural segmentation is that this task is regarded as both ambiguous (i.e., there may be more than one valid annotation for a given track~\cite{McFee2017}) and subjective (i.e., two different listeners might perceive different sets of segment boundaries~\cite{Bruderer2009}).
This has led to different methodologies when annotating and gathering structural datasets, thus having a diverse ecosystem of sets to choose from when evaluating automatic approaches.

The first time this task appeared on MIREX was in 2009,\footnote{https://www.music-ir.org/mirex/wiki/2009:Structural\_Segmentation} where annotations from The Beatles dataset (included in the set described above) and a subset of the Real World Computing Popular Music Database (RWC)~\cite{Goto2002} were employed.
These sets contain several functional segment annotations for western (The Beatles) and Japanese (RWC) popular music.
These segment functions describe the \emph{purpose} of the segments, e.g.: ``solo'', ``verse'', ``chorus.''
A single annotation per track is provided for these two sets.
The Beatles dataset was further revised at the Tampere University of Technology,\footnote{http://www.cs.tut.fi/sgn/arg/paulus/beatles\_sections\_TUT.zip} and additional functional segment annotations for other other bands were added to The Beatles set, which became known as the Isophonics Dataset~\cite{Mauch2009a}.
No beat or downbeat annotations were provided to the rest of the tracks in Isophonics, and the final number of tracks with functional structural segment annotations is 300.
The number of annotated tracks in the RWC is 365.

To address the open problems of ambiguity and subjectivity, further annotations per track from several experts could be gathered.
That is the case with The Structural Annotations for Large Amounts of Music Information (SALAMI) dataset~\cite{Smith2011}, where most of its nearly 1,400 tracks have been annotated by at least 2 humans.
Similarly, the Structural Poly Annotations of Music (SPAM) dataset~\cite{Nieto2016} provides 5 different annotations for 50 tracks.
These two sets not only contain functional levels of annotations, but also large and small scale segments where only single letters describing the similarity between segments are annotated.
Thus, these can be seen as sets that contain \emph{hierarchical} data, which remains largely unexploited in the MIREX competition~\cite{Ehmann2011, Smith2013}.
As opposed to Isophonics and RWC, these two sets contain highly diverse music in terms of genre: from world-music to rock, including jazz, blues, and live music.

The following properties typically define segmentation datasets:
\textbf{Number of annotators}: This can help when trying to quantify the amount of disagreement among annotators~\cite{McFee2017,Nieto2016}, or when developing approaches that may yield more than one potentially valid segmentation.
\textbf{Hierarchy}: The levels of annotations contained in the set. It typically contains functional, large, and/or small segment annotations.
When only when level of annotations is provided, these are typically called \emph{flat} segment annotations.


\section{The \setNameUpper~Set}\label{sec:dataset}

In this section we present the \setName~set, including the methodology of acquiring the data, its motivation, its contents, and a set of statistics of its annotations.
The set is publicly available on-line.\footnote{URL not displayed for the blind review process.}

\subsection{Data Gathering}

How were the songs selected. Show genre distribution plot.

Who annotated them (e.g., professional musicians?)

How they annotated them (tools, revisions, etc.). Why only flat segmentation.

Why were the songs selected (motivation behind \setName).

Additional data could be obtained by data augmentation~\cite{Mcfee2015}.

\subsection{Dataset Contents}

The \setName~set contains manual annotations for 715 western popular music tracks, thus being the largest published to date dataset containing beats, downbeats, and function structural segmentation information.
The annotations and some of the song-level metadata are distributed via JAMS~\cite{Humphrey2014} files, one per track.
This format is chosen given its simplicity when storing multi-task annotations plus song- and corpus-level metadata.
Each JAMS file contains the beat, downbeat, and functional segmentation annotations, plus a set of estimated onsets for the first 30 seconds of the audio.
These onsets should help aligning the audio in case researchers obtain audio data with different compression formats that might include certain small time offsets in the time signal.
This onset information was computed using \texttt{librosa}~\cite{Mcfee2015a} version TODO, with their default parameters.

For the sake of transparency and usability, we also publish the raw beats, downbeats, and segmentation data as space-separated text files, two per track: one for beats and downbeats, and the other for segments.
We also distribute the code that converts these raw annotations into unified JAMS file.

Furthermore, we provide other identifiers with the aim of easily retrieving additional metadata and/or audio content for each song.
These identifiers include:

\begin{itemize}
    \item \textbf{MusicBrainz}\footnote{https://musicbrainz.org/}: open music encyclopedia including unique identifiers for recordings, releases, artists, etc. 
    \item \textbf{AcoustID}\footnote{https://acoustid.org/}: open source fingerprinting service to easily match audio content, typically associated with MusicBrainz identifiers.
\end{itemize}

Finally, we provide a single csv file including additional metadata information such as genre and proposed train/test splits.
These splits are defined by retaining the genre distribution both in the training and test sets, with a 80\% and 20\% distributions of the tracks, respectively.

\subsection{Data Statistics}

In this subsection we provide several data insights obtained from the annotations to give an objective overview of the set.
In Figure~\ref{fig:BPM_dist} we show the estimated tempo distribution (beats-per-minute or BPM) per track.
These estimations were computed using the track-level median beat length for each of the annotated beats in a given track.
There is a clear peak at 128 BPM, which could be explained by being the most common tempo in electronic dance music~\cite{Moelants2008}.
Furthermore, in Figure~\ref{fig:BPM_std} we plot the standard deviation of the estimated tempo.
We can clearly see that the tempo is remarkably steady in this dataset, which is expected given the type of musical genres it spans.


\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/BPM_distribution.pdf}}
    \caption{Tempo distribution of the tracks in the \setName~set.}
    \label{fig:BPM_dist}
\end{figure}

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/BPM_std.pdf}}
    \caption{Standard deviation of the tempo distribution in the \setName~set.}
    \label{fig:BPM_std}
\end{figure}

In terms of segment statistics, we show data based on certain attributes described in a MIREX meta-analysis of the segmentation task~\cite{Smith2013}.
In Figure~\ref{fig:segment_count} we plot track-level histograms for the number of segments, and the number of unique segments (i.e., those with the same associated label).
Both distributions seem to be unimodal and centered around 10 and 11 for the number of segments per tracks, and around 6 and 7 for the number of unique labels per track.
This differs from the number of unique segments in The Beatles dataset, which is centered around 4 per track~\cite{Nieto2014}.

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/segment_label_count.pdf}}
    \caption{Number of segments per track, based on their segment labels.}
    \label{fig:segment_count}
\end{figure}

Figure~\ref{fig:seglabel_dist} shows the frequency in which the most common segment labels appear in the set.
The labels ``chorus'' and ``verse'' dominate the distribution, as these functional parts are common in western popular music.
The plot also shows potentially repeated labels like ``inst'' and ``instrumental.''
A further inter-song analysis of the labels might be necessary to potentially merge certain labels and thus unify the vocabulary of the set.

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/SegmentLabels_distribution.pdf}}
    \caption{Most common segment labels.}
    \label{fig:seglabel_dist}
\end{figure}

We plot in Figure~\ref{fig:seglen_dist} the distribution of the segment lengths, in seconds, across the entire dataset.
It might be interesting to discuss the peak at 15 seconds.
As we showed in Figure~\ref{fig:BPM_dist}, there is a majority of tracks at 128 BPM, which makes 15 seconds a segment of exactly 32 beats.
This, in the common 4/4 time signature, would result in 8 bars per each 15-second segment in that tempo, and 8 bars are rather usual in electronic dance music~\cite{Moelants2008}.

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/SegmentLength_distribution.pdf}}
    \caption{Segment length distribution.}
    \label{fig:seglen_dist}
\end{figure}

Finally, and thanks to having access to the annotated downbeats, we show in Figure~\ref{fig:downbeat_segment} the number of segments starting at a specific beat within a given bar.
We can see that the vast majority of segments (81.1\%) start in a downbeat.
Interestingly, several segments (10\%) start in position 4, thus showing that 1-beat count-ins are more common than other types of count-ins on this dataset.

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/Downbeat_Segment_Alignment.pdf}}
    \caption{Number of segments based on their starting beat position within a bar.}
    \label{fig:downbeat_segment}
\end{figure}

\section{Results}\label{sec:results}

\subsection{Beat Results}

In order to establish performance baselines over the dataset for the task of beat-tracking. We have evaluated a number of openly available beat
tracking algorithms on the dataset \cite{Ellis2007, Krebs2015, Korzeniowski2014, Bock2011}. Each of these algorithms can be found in either the
Madmom \cite{Bock2016b} or Librosa \cite{Mcfee2015a} python libraries. This allows a comparison between datasets for which these algorithms have been previously evaluated,
with respect to their affect on these algorithm's performance. The results are also provided with the dataset in CSV format. This is intendended
as a convenience for any future work that wishes to evaluate novel algorithms against these benchmarks.

The beat tracking results for the aforementioned algorithms are displayed in Figure~\ref{fig:beat_results}. There they are evaluated across two metrics,
F-Measure, and Max~F-Measure, where the latter refers to the maximum F-Measure obtained per track when evaluated across double and half-time metrical variations
in the annotated beats provided with this dataset. For half-time metrical variations, both the downbeat and offbeat alignments were tested for a maximum F-Measure
value. While \cite{Ellis2007} is the most computationally efficient of the algorithms, we see clear gains in the later methods developed. It can be seen that with 
this dataset, both \cite{Ellis2007} and the ``BeatDetector'' algorithm from \cite{Bock2011} when using  and REF have a significant number of double-half time errors, compared to REFS.
Unlike the ``BeatTracker'' algorithm in \cite{Bock2011}, the ``BeatDetector'' algorithm assumes constant tempo.

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/2019_04_09_19_15_00_Beat_TrackiMax_F-Measure.pdf}}
    \caption{Beat tracking performance over the \setName~set, for the algorithms Ellis \cite{Ellis2007}, Krebs \cite{Krebs2015}, Korzeniowski \cite{Korzeniowski2014}, B{\"o}ck 1 - the ``BeatDetector'' technique from \cite{Bock2011}, and B{\"o}ck 1 - the ``BeatTracker'' technique from \cite{Bock2011}.}
    \label{fig:beat_results}
\end{figure}

\subsection{Downbeat Results}

Unfortunately the availability of open source downbeat estimation libraries is limited. In order to provide a baseline for downbeat detection performance
with the \setName~set specifically, results have been evaluated with the downbeat detection algorithms available in \cite{Bock2016b}. Two algorithms were
evaluated including the method proposed in \cite{Bock2016a}, and the dynamic Bayesian bar tracking processor using the input from the RNN bar processor activation
function - all of which is available within the Madmom package \cite{Bock2016b}. The results can be seen in Figure~\ref{fig:downbeat_results}. For the method in \cite{Bock2016a}, the annotated beats are provided from which
to estimate the downbeats. The significant improvement in performance obtained by this algorithm over that with the estimated beat positions highlights the importance
of reliable beat tracking for downbeat estimation performance, and the interdependence between the beat tracking and downbeat estimation tasks.

\begin{figure}
    \centerline{\includegraphics[width=\columnwidth]{figs/2019_04_09_10_27_35_Downbeat_Re_F-Measure.pdf}}
    \caption{Downbeat tracking performance over the \setName~set, for the algorithms B{\"o}ck 1 \cite{Bock2016a} and B{\"o}ck 2 - a dynamic Bayesian network provided within the Madmom package \cite{Bock2016b}.}
    \label{fig:downbeat_results}
\end{figure}

\subsection{Segmentation Results}

Segmentation results. Using~\cite{Nieto2016}. Serr\`a algorithm~\cite{Serra2014}. Nieto algorithm~\cite{Nieto2014}. Results included in CVS format for all dataset.


\begin{figure}
    \centerline{\includegraphics[width=1.1\columnwidth]{figs/segment_results.pdf}}
    \caption{Segmentation results}
    \label{fig:segment_results}
\end{figure}

\subsection{Beat + Segmentation Results}

Beat + Segmentation.

\section{Conclusions}\label{sec:conclusions}

We presented the \setName~set, the largest dataset in terms of human annotations containing the following three types of music information: beats, downbeats, and function structural segments.

TODO: Potential of combined data + large amounts of annotations.

% For bibtex users:
\bibliography{ISMIRtemplate,beat_data_refs}

\end{document}
